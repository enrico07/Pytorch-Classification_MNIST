{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzJlwk4X7UC8"
      },
      "source": [
        "# **CLASSIFICATION TASK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzmLqXlL7GCK"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB8PP_DNE6vQ"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhNZGH57E_fI"
      },
      "source": [
        "train_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\n",
        "test_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltbPTyXgFqFz"
      },
      "source": [
        "# **Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2GhV-qcFpIh"
      },
      "source": [
        "class CNN(nn.Module):  #CNN definition: 2 convolutional layer, 2 linear layer\n",
        "    def __init__(self, C1, C2, Ni, Nh1, No):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(Ni, C1, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(C1, C2, kernel_size=3)\n",
        "        self.conv2_drop = nn.Dropout2d() #dropout\n",
        "        self.batchnorm1 = nn.BatchNorm2d(C1) #batch normalization\n",
        "        self.batchnorm2 = nn.BatchNorm2d(C2)  #batch normalization\n",
        "        self.m = nn.Softmax(dim=1) #softmax function\n",
        "        self.fc1 = nn.Linear(1600, Nh1)\n",
        "        self.fc2 = nn.Linear(Nh1, No)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batchnorm1(F.relu(F.max_pool2d(self.conv1(x), 2)))\n",
        "        x = self.batchnorm2(F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)))\n",
        "        x = x.view(x.shape[0],-1) #flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.m(self.fc2(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ3sqElfjNqa"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crJwj3FNjP_p"
      },
      "source": [
        "class ClassificationDataset(Dataset): #dataset definition\n",
        "\n",
        "  def __init__(self, dataset, transform=None):\n",
        "    self.transform = transform\n",
        "    \n",
        "    self.data = []\n",
        "    for elem in dataset: #each element is added to a list\n",
        "      self.data.append(elem)\n",
        "\n",
        "  def __len__(self): #lenght of the dataset\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx): #Function for retreiving elements. It receives a list of integers as input\n",
        "    elements = []\n",
        "\n",
        "    for elem in idx: #For each integer in the input list, retrives the element in that position\n",
        "      sample = self.data[elem]\n",
        "      if self.transform:\n",
        "          sample = self.transform(sample)\n",
        "      \n",
        "      elements.append(sample)\n",
        "    return elements #return list of selected elements\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample): #image transformation to tensor\n",
        "        x, y = sample\n",
        "        return (transforms.ToTensor()(x).float(), y)\n",
        "\n",
        "class Normalization(object):\n",
        "    def __call__(self, sample): #image normalization with mean 0.5 and std 0.5\n",
        "        x, y = sample\n",
        "        return (transforms.Normalize(0.5, 0.5)(x).float(), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGJq5VXZlsQi"
      },
      "source": [
        "composed_transform = transforms.Compose([ToTensor(), Normalization()])\n",
        "\n",
        "train_data = ClassificationDataset(train_dataset, transform=composed_transform) #train dataset\n",
        "test_data = ClassificationDataset(test_dataset, transform=composed_transform) #test dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G14sxntiSIlr"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jKwZ1R5SMHq"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Training device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pb7dDR-SPZL"
      },
      "source": [
        "def initNet(): #function that initializes a new NN\n",
        "  torch.manual_seed(0)\n",
        "  Ni = 1 #number of input neurons\n",
        "  C1 = 10 #number of the channel first convolutional layer\n",
        "  C2 = 64 #number of the channel second convolutional layer\n",
        "  Nh1 = 128 #number of neurons of the first linear layer\n",
        "  No = 10 #number of neurons of output layer\n",
        "  net = CNN(C1, C2, Ni, Nh1, No)\n",
        "  net.to(device)\n",
        "\n",
        "  return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MQqDzfjcCNV"
      },
      "source": [
        "def get_optimizer(n, net, lr): #Function for choosing the optimizer: n = index for the list, net = network model, lr = learning rate\n",
        "  typeM = [optim.Adagrad(net.parameters(), lr=lr, weight_decay = 1e-3), optim.Adadelta(net.parameters(), lr = lr, weight_decay = 1e-3), optim.Adam(net.parameters(), lr=lr, weight_decay = 1e-3), optim.RMSprop(net.parameters(), lr=lr, weight_decay = 1e-3)]\n",
        "\n",
        "  return typeM[n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf93CzJRfZq5"
      },
      "source": [
        "###########\n",
        "#K-Fold Cross Validation\n",
        "###########\n",
        "\n",
        "nIter = 2 #number of different set of hyperparameters to try\n",
        "model_set = {} #best hyperparameters\n",
        "loss_fn = nn.CrossEntropyLoss() #cross-entropy loss function\n",
        "\n",
        "for i in range(nIter): #for the number of different set\n",
        "  print('#################')\n",
        "  print(f'# Iter {i}')\n",
        "  print('#################')\n",
        "\n",
        "  kf = KFold(n_splits=5) #5-fold cross validation\n",
        "  num_epochs = random.randrange(100, 300, 20) #number of iteration for training the model\n",
        "  lr = random.uniform(0, 0.01) #learning rate\n",
        "  batch_num = [] #batch size\n",
        "  type_optimizer = random.randrange(0, 4) #type of optimizer (i. e. index in the dictionary for choosing the optimizer)\n",
        "  \n",
        "  train_loss_Fold = [] #list for saving the training loss for each fold\n",
        "  val_loss_Fold = [] #list for saving the validation loss for each fold\n",
        "\n",
        "  for train_index, val_index in kf.split(train_data): #for each fold\n",
        "    print('#################')\n",
        "    print(f'# Fold')\n",
        "    print('#################')\n",
        "    net = initNet()\n",
        "    optimizer = get_optimizer(type_optimizer, net, lr)\n",
        "\n",
        "    trainSet, valSet = train_data.__getitem__(train_index), train_data.__getitem__(val_index) #division in training set and validation set\n",
        "\n",
        "    if batch_num == []: #check if it's the first iteration\n",
        "      batch_num = random.randrange(1, 1000)\n",
        "\n",
        "    trainSetX = DataLoader(trainSet, batch_size= batch_num, shuffle=True, num_workers=0) #dataloader of the training set\n",
        "    valSetX = DataLoader(valSet, batch_size=len(valSet), shuffle=True, num_workers=0) #dataloader of the validation set\n",
        "\n",
        "    train_loss= [] #list for saving the training loss at each epoch\n",
        "    val_loss= [] #list for saving the validation loss at each epoch\n",
        "\n",
        "    for epoch_num in range(num_epochs): #for each epoch\n",
        "      \n",
        "      net.train() #training\n",
        "      for sample_batched in trainSetX: #for each batch\n",
        "        x_batch = sample_batched[0].to(device) #input elements\n",
        "        label_batch = sample_batched[1].to(device) #labels\n",
        "\n",
        "        out = net(x_batch) #output of the model\n",
        "\n",
        "        loss = loss_fn(out, label_batch) #loss of the model\n",
        "        \n",
        "        net.zero_grad()\n",
        "        loss.backward() #backprobagation\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch_num == num_epochs - 1: #saving the loss at the last epoch\n",
        "          loss_batch = loss.detach().cpu().numpy()\n",
        "          train_loss.append(loss_batch)\n",
        "\n",
        "      net.eval() #evaluation\n",
        "      with torch.no_grad():\n",
        "        for sample_batched in valSetX: #for each batch\n",
        "          x_batch = sample_batched[0].to(device) #input elements\n",
        "          label_batch = sample_batched[1].to(device) #labels\n",
        "\n",
        "          out = net(x_batch) #output of the model\n",
        "\n",
        "          loss = loss_fn(out, label_batch) #loss of the model\n",
        "\n",
        "          if epoch_num == num_epochs - 1: #saving the loss at the last epoch\n",
        "            loss_batch = loss.detach().cpu().numpy()\n",
        "            val_loss.append(loss_batch)\n",
        "      \n",
        "    train_loss = np.mean(train_loss) #mean loss of different batches\n",
        "    train_loss_Fold.append(train_loss)\n",
        "\n",
        "    val_loss = np.mean(val_loss) #mean loss of different batches\n",
        "    val_loss_Fold.append(val_loss)\n",
        "\n",
        "  train_loss = np.mean(train_loss_Fold) #mean of the loss of each fold\n",
        "  print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
        "\n",
        "  val_loss = np.mean(val_loss_Fold) #mean of the loss of each fold\n",
        "  print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
        "  val_loss_Fold.append(val_loss)\n",
        "\n",
        "  if len(model_set) == 0 or val_loss < model_set[\"loss\"]: #save the new best hyperparameter if the new validation losso is lower\n",
        "    model_set[\"num_epochs\"] = num_epochs\n",
        "    model_set[\"lr\"] = lr\n",
        "    model_set[\"num_batch\"] = batch_num\n",
        "    model_set[\"type_opt\"] = type_optimizer\n",
        "    model_set[\"loss\"] = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrU-eAgYtGir"
      },
      "source": [
        "In the next cell you can choose to use whether the hyperparameters that I found to be good or the set of hyperparameters that you found during cross validation\n",
        "\n",
        "Set 'use_saved' equal to True if you want to use the set that I found"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xNxGR7yB2v7"
      },
      "source": [
        "hyper_set = {} #set of the best hyperparameters\n",
        "use_saved = True #true if we want to use a set of hyperparameters that is already checked to be very good\n",
        "                #false if we want to use the new set of hyperparameters given by cross validation\n",
        "if use_saved:\n",
        "  hyper_set = {'lr': 0.0096, 'num_batch': 123, 'num_epochs': 150, 'type_opt': 0}\n",
        "else:\n",
        "  hyper_set = {'lr': model_set[\"lr\"], 'num_batch': model_set[\"num_batch\"], 'num_epochs': model_set[\"num_epochs\"], 'type_opt': model_set[\"type_opt\"]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKRH3f6xgB-S"
      },
      "source": [
        "num_epochs = hyper_set[\"num_epochs\"] #number of epochs\n",
        "lr = hyper_set[\"lr\"] #larning rate\n",
        "batch_num = hyper_set[\"num_batch\"] #size of the batch\n",
        "type_optimizer = hyper_set[\"type_opt\"] #type of otpimizer\n",
        "\n",
        "train_loss_log = [] #list of training error at each epoch\n",
        "test_loss_log = [] #list of validation error at each epoch\n",
        "\n",
        "train_dataloader = DataLoader(train_data.__getitem__(range(len(train_data))), batch_size=batch_num, shuffle=True, num_workers=0)\n",
        "\n",
        "net = initNet() #initialization of the nework\n",
        "loss_fn = nn.CrossEntropyLoss() #loss function\n",
        "optimizer = get_optimizer(type_optimizer, net, lr) #optimizer\n",
        "\n",
        "for epoch_num in range(num_epochs): #for each epoch\n",
        "  print('#################')\n",
        "  print(f'# EPOCH {epoch_num}')\n",
        "  print('#################')\n",
        "\n",
        "  train_loss= []\n",
        "  net.train()\n",
        "  for sample_batched in train_dataloader: #for each batch\n",
        "    x_batch = sample_batched[0].to(device)\n",
        "    label_batch = sample_batched[1].to(device)\n",
        "\n",
        "    out = net(x_batch)\n",
        "\n",
        "    loss = loss_fn(out, label_batch)\n",
        "\n",
        "    net.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_batch = loss.detach().cpu().numpy()\n",
        "    train_loss.append(loss_batch)\n",
        "    \n",
        "  train_loss = np.mean(train_loss) #mean training loss of the batches\n",
        "  print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
        "  train_loss_log.append(train_loss)\n",
        "\n",
        "\n",
        "#Save the trained network\n",
        "\n",
        "#net_state_dict = net.state_dict()\n",
        "#torch.save(net_state_dict, 'Classification.torch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMrkslLgrAlf"
      },
      "source": [
        "#######\n",
        "#Plot of the evolution of the training error\n",
        "#######\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.semilogy(train_loss_log, label='Train loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKaDepL4EDjZ"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOXLu1ldtKug"
      },
      "source": [
        "Set 'use_pre' equal to True in the next cell to load a pretrained network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjRwDpJbeV5i"
      },
      "source": [
        "#Load a previus trained network\n",
        "use_pre = False\n",
        "\n",
        "if use_pre:\n",
        "  net = initNet() \n",
        "  # Load the state dict previously saved\n",
        "  net_state_dict = torch.load('Classification.torch')\n",
        "  # Update the network parameters\n",
        "  net.load_state_dict(net_state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfSXcoa-zDgk"
      },
      "source": [
        "#######\n",
        "#Testing of the Model on Test Dataset\n",
        "#######\n",
        "\n",
        "test_dataloader = DataLoader(test_data.__getitem__(range(len(test_data))), batch_size=len(test_data), shuffle=False, num_workers=0)\n",
        "\n",
        "val_loss= [] \n",
        "net.eval() #evaluation\n",
        "with torch.no_grad(): #disable gradient tracking\n",
        "  for sample_batched in test_dataloader:\n",
        "    x_batch = sample_batched[0].to(device)\n",
        "    label_batch = sample_batched[1].to(device)\n",
        "\n",
        "    out = net(x_batch)\n",
        "\n",
        "    loss = loss_fn(out, label_batch)\n",
        "\n",
        "    loss_batch = loss.detach().cpu().numpy()\n",
        "    val_loss.append(loss_batch)\n",
        "  val_loss = np.mean(val_loss) #mean of the loss of the batches\n",
        "  print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-9uFL2g0eWd"
      },
      "source": [
        "#Computation of the accuracy on test dataset\n",
        "\n",
        "count = 0\n",
        "\n",
        "for elem, pred_labels in zip(test_data.data, out): #for each element in the test data and output of the model\n",
        "  label_indx = pred_labels.argmax(0) #class \n",
        "  true_label = elem[1] #true label\n",
        "\n",
        "  if label_indx == true_label: #if true and predicted are equal\n",
        "    count += 1\n",
        "\n",
        "print(f\"ACCURACY: {count * 100 / len(test_data)}%\") #percentage of true predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkS0EF4f3rbR"
      },
      "source": [
        "#######\n",
        "#Testing of the Model on Training Dataset\n",
        "#######\n",
        "\n",
        "trainL = DataLoader(train_data.__getitem__(range(len(train_data))), batch_size=len(train_data), shuffle=False, num_workers=0)\n",
        "\n",
        "val_loss= []\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "  for sample_batched in trainL:\n",
        "    x_batch = sample_batched[0].to(device)\n",
        "    label_batch = sample_batched[1].to(device)\n",
        "\n",
        "    out_train = net(x_batch)\n",
        "\n",
        "    loss = loss_fn(out_train, label_batch)\n",
        "\n",
        "    loss_batch = loss.detach().cpu().numpy()\n",
        "    val_loss.append(loss_batch)\n",
        "  val_loss = np.mean(val_loss)\n",
        "  print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
        "\n",
        "#Computation of the accuracy on test dataset\n",
        "\n",
        "count = 0\n",
        "\n",
        "for elem, pred_labels in zip(train_data.data, out_train):\n",
        "  label_indx = pred_labels.argmax(0)\n",
        "  true_label = elem[1]\n",
        "\n",
        "  if label_indx == true_label:\n",
        "    count += 1\n",
        "\n",
        "print(f\"ACCURACY: {count * 100 / len(train_data)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUkWTukxtRiw"
      },
      "source": [
        "Set 'n' in the next cell to choose the number of picture to display\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49b8ZvY625jj"
      },
      "source": [
        "#Print of model prediction\n",
        "n = 10 #number of picture to display\n",
        "for elem,pred_labels, i, k in zip(test_data.data, out, range(len(out)), range(n)):\n",
        "  label_indx = pred_labels.argmax(0)\n",
        "  im = elem[0]\n",
        "\n",
        "  print(f\"LABEL {i +1}: {label_indx}\")\n",
        "  fig = plt.figure(figsize=(8,8))\n",
        "  plt.imshow(im, cmap='Greys')\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsYnTb0Rmsvp"
      },
      "source": [
        "#Missclassified  images\n",
        "\n",
        "for elem,pred_labels, i in zip(test_data.data, out, range(len(out))):\n",
        "  label_indx = pred_labels.argmax(0)\n",
        "  im = elem[0]\n",
        "  true_label = elem[1]\n",
        "\n",
        "  if label_indx !=  true_label:\n",
        "    print(f\"True Label: {true_label}\")\n",
        "    print(f\"Predicted Label: {label_indx}\")\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    plt.imshow(im, cmap='Greys')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl8EmjgxGIfZ"
      },
      "source": [
        "# **Weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6F_ReK3dqpO"
      },
      "source": [
        "#First convolutional layer\n",
        "h1_w = net.conv1.weight.data.cpu().numpy()\n",
        "\n",
        "#Second convolutional layer\n",
        "h2_w = net.conv2.weight.data.cpu().numpy()\n",
        "\n",
        "#First linear layer\n",
        "h3_w = net.fc1.weight.data.cpu().numpy()\n",
        "h3_b = net.fc1.bias.data.cpu().numpy()\n",
        "\n",
        "#Output layer\n",
        "out_w = net.fc2.weight.data.cpu().numpy()\n",
        "out_b = net.fc2.bias.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSdxWMa8-jpC"
      },
      "source": [
        "#######\n",
        "#Plotting of the Weights Histograms\n",
        "########\n",
        "\n",
        "fig, axs = plt.subplots(4, 1, figsize=(12,8))\n",
        "axs[0].hist(h1_w.flatten(), 50)\n",
        "axs[0].set_title('First convolutional layer weights')\n",
        "axs[1].hist(h2_w.flatten(), 50)\n",
        "axs[1].set_title('Second convolutional layer weights')\n",
        "axs[2].hist(h3_w.flatten(), 50)\n",
        "axs[2].set_title('First linear layer weights')\n",
        "axs[3].hist(out_w.flatten(), 50)\n",
        "axs[3].set_title('Second linear layer weights')\n",
        "[ax.grid() for ax in axs]\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDqnbuqS_hAm"
      },
      "source": [
        "# **Activations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbbhgevL_Xg8"
      },
      "source": [
        "def get_activation(layer, input, output):\n",
        "    global activation\n",
        "    activation = torch.sigmoid(output)\n",
        "\n",
        "#register hook \n",
        "\n",
        "hook_handle = net.fc1.register_forward_hook(get_activation)\n",
        "\n",
        "#analyze activations\n",
        "net = net.to(device)\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    x1 = test_data.__getitem__([0])[0][0].unsqueeze(0).to(device) #first image of the test dataset\n",
        "    y1 = net(x1)\n",
        "    z1 = activation\n",
        "    x2 = test_data.__getitem__([-1])[0][0].unsqueeze(0).to(device) #last image of the test dataset\n",
        "    y2 = net(x2)\n",
        "    z2 = activation\n",
        "\n",
        "#remove hook\n",
        "hook_handle.remove()\n",
        "\n",
        "#plot activations\n",
        "fig, axs = plt.subplots(2, 1, figsize=(12,6))\n",
        "axs[0].stem(z1[0].cpu().numpy(), use_line_collection=True)\n",
        "axs[0].set_title('First linear layer activations for the first image of the test set')\n",
        "axs[1].stem(z2[0].cpu().numpy(), use_line_collection=True)\n",
        "axs[1].set_title('First linear layer activations for the last image of the test set')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAaAmXnI8cK-"
      },
      "source": [
        "# **Filters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhoeS4eT3P7D"
      },
      "source": [
        "model_weights = [] #list the conv layer weights in this list\n",
        "conv_layers = [] # list of convulational layer\n",
        "\n",
        "model_children = list(net.children()) #list of the model children\n",
        "\n",
        "# append all the conv layers and their respective weights to the list\n",
        "for i in range(len(model_children)): #for each children\n",
        "    if type(model_children[i]) == nn.Conv2d: #check if the layer is a convulational layer\n",
        "        model_weights.append(model_children[i].weight)\n",
        "        conv_layers.append(model_children[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4byEsX7g3oXo"
      },
      "source": [
        "######\n",
        "#Print the weighs of the filters\n",
        "######\n",
        "\n",
        "print(\"Weights of Convolutonal Layer #1\")\n",
        "plt.figure(figsize=(20, 7))\n",
        "for i, filter in enumerate(model_weights[0]):\n",
        "  filter = filter.cpu()\n",
        "  plt.subplot(2, 5, i+1)\n",
        "  plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
        "  plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"Weights of Convolutonal Layer #2\")\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, filter in enumerate(model_weights[1]):\n",
        "  filter = filter.cpu()\n",
        "  plt.subplot(8, 8, i+1)\n",
        "  plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UP2JBdwG_Yg"
      },
      "source": [
        "# **Feature Maps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO9lTU4n9ZYC"
      },
      "source": [
        "img = test_data.__getitem__([0])[0][0].unsqueeze(0).to(device) #first image of the test dataset\n",
        "\n",
        "results = [conv_layers[0](img)] #passing the image to the first convulational layer\n",
        "for i in range(1, len(conv_layers)): #passing the results of the previus convulatinal layer to the next one\n",
        "  results.append(conv_layers[i](results[-1]))\n",
        "\n",
        "outputs = results #results\n",
        "\n",
        "#Visualize the feture maps of the filters\n",
        "\n",
        "for num_layer in range(len(outputs)): #for each layer\n",
        "  print(f\"Convolutional layer #{num_layer + 1}\")\n",
        "  plt.figure(figsize=(30, 30))\n",
        "  layer_viz = outputs[num_layer][0, :, :, :] #values of a particular layer\n",
        "  layer_viz = layer_viz.data\n",
        "  for i, filter in enumerate(layer_viz): #for each filter\n",
        "    filter = filter.cpu()\n",
        "    plt.subplot(8, 8, i + 1)\n",
        "    plt.imshow(filter, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}